{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilmer-1/DM23/blob/main/DMA_22_Lab_7__Dimensionality_Reduction_and_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDlU-kLCKDVZ"
      },
      "source": [
        "NAME = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW9zL4V6KDVc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9a0ec075584699a44c46933457b0a8ba",
          "grade": false,
          "grade_id": "cell-a910b376742d04c0",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ECD5r2hFKDVd"
      },
      "source": [
        "# Lab 7: Dimensionality Reduction\n",
        "\n",
        "**Please read the following instructions very carefully**\n",
        "\n",
        "## Working on the assignment / FAQs\n",
        "- **Always use the seed/random_state as *42* wherever applicable** (This is to ensure repeatability in answers, across students and coding environments)\n",
        "- The type of question and the points they carry are indicated in each question cell\n",
        "- To avoid any ambiguity, each question also specifies what *value* must be set. Note that these are dummy values and not the answers\n",
        "- If an autograded question has multiple answers (due to differences in handling NaNs, zeros etc.), all answers will be considered.\n",
        "- You can delete the `raise NotImplementedError()`\n",
        "- **Submitting the assignment** : Download the '.ipynb' file and the PDF file from Colab and upload it to Gradescope. Do not delete any outputs from cells before submitting.\n",
        "- That's about it. Happy coding!\n",
        "\n",
        "\n",
        "Available software:\n",
        " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
        " - Sklearn’s  TSNE module in case you use TSNE to reduce dimension (optional)\n",
        " - Python’s Matplotlib (optional)\n",
        "\n",
        "_Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a09a0bf3042da711c4bf843e9b4a4189",
          "grade": false,
          "grade_id": "cell-bf780e597c0216c8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Vsocwry-KDVe",
        "outputId": "2e104888-4370-4bd3-f169-8e2fa853a49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gensim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import requests\n",
        "import string\n",
        "\n",
        "from IPython.display import Image\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# To make the visualizations\n",
        "!git clone https://github.com/CAHLR/d3-scatterplot.git\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import Javascript\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# To download trained model (Google news)\n",
        "import gensim.downloader as api\n",
        "google_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Cloning into 'd3-scatterplot'...\n",
            "remote: Enumerating objects: 1068, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 1068 (delta 32), reused 0 (delta 0), pack-reused 1016\u001b[K\n",
            "Receiving objects: 100% (1068/1068), 1.97 MiB | 10.97 MiB/s, done.\n",
            "Resolving deltas: 100% (621/621), done.\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "47031c66b74746d23ccc5e8369446a4b",
          "grade": false,
          "grade_id": "cell-3f89500615a0096f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ZF74G9bDKDVh"
      },
      "source": [
        "### **Q1** (0.25 points)\n",
        "Download your text corpus. (A good place to start is the [nltk corpus](http://www.nltk.org/nltk_data/) or the [gutenberg project](https://www.gutenberg.org/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoqPhRZrmAVT"
      },
      "source": [
        "#your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "4d52dda406c3d8cd5e37d29755f0fb12",
          "grade": false,
          "grade_id": "cell-fbbe575f8f5a6368",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "SZD5ZaMvKDVk"
      },
      "source": [
        "#Save the raw text that you just downloaded in this variable\n",
        "raw = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "569aa8b664a41d901bf7b0a5e23e9930",
          "grade": true,
          "grade_id": "cell-929d59ed5d67f618",
          "locked": true,
          "points": 1,
          "schema_version": 1,
          "solution": false
        },
        "id": "tFUPLSK7KDVp"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "print(raw[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a7f270405ddf9ecbffde36e6c096b818",
          "grade": false,
          "grade_id": "cell-ccd6618b4fac3715",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ZcqpWCjJKDVs"
      },
      "source": [
        "### **Q2** (0.25 points)\n",
        "Tokenize your corpus. Make sure that that the result is a list of lists i.e. The top-level list (outer list) is a list of sentences, and the inner list is a list of words in a given sentence.\n",
        "\n",
        "Consider the following text:\n",
        "\n",
        "```\n",
        "text = \"I spent $15.35 on my lunch today. Food in Berkeley is very expensive!\"\n",
        "```\n",
        "\n",
        "It could be tokenized as follows:\n",
        "\n",
        "```\n",
        "tok_corp = [['I', 'spent', '$', '15.35', 'on', 'my', 'lunch', 'today'],\n",
        " ['Food', 'in', 'Berkeley', 'is', 'very', 'expensive']]\n",
        "```\n",
        "\n",
        "\n",
        "Note: There are many different (and correct) ways of tokenizing. Your answer doesn't need to match exactly with this illustrative example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGmhMhInSyoa"
      },
      "source": [
        "#code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "6b3cecb268eb9440c446cd3de984b7f6",
          "grade": false,
          "grade_id": "cell-00f3d05abb28aa23",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "6pzKlLyjKDVt"
      },
      "source": [
        "#Save the tokenized sentences as a list of list in this variable\n",
        "tok_corp = [[]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "401940f859774b3c1ec48338fa15682e",
          "grade": true,
          "grade_id": "cell-6f34229370fa873f",
          "locked": true,
          "points": 1,
          "schema_version": 1,
          "solution": false
        },
        "id": "Hkj2ROGTKDVv"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "for sent in tok_corp[:3]:\n",
        "  print(sent)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ac8b42811c924e7988f17b9dbd3f71ef",
          "grade": false,
          "grade_id": "cell-4ad44071d3785409",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "2UBnMwiXKDVy"
      },
      "source": [
        "### **Q3** (0.25 points)\n",
        "Train gensim using your own dataset. Name the trained model variable as `model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mErVl87SPbNV"
      },
      "source": [
        "#code here\n",
        "model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkF8Q2JNPbAO"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "print(f'Corpus Size: {model.corpus_total_words}')\n",
        "print(f'Corpus Count: {model.corpus_count}')\n",
        "print(f'Training time: {model.total_train_time}')\n",
        "print(f'Sample words: {list(model.wv.index_to_key[:10])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ6siFPKRor7"
      },
      "source": [
        "### **Q4** (0.25 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFCe5MzG4CEU"
      },
      "source": [
        "\n",
        "#### **Q4a**\n",
        "\n",
        "Create a list of the unique set of words from your corpus. Name the list variable as `unique_words`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgKhbEV4CBU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goHFGMzsR_nu"
      },
      "source": [
        "#code here\n",
        "unique_words = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3oN4cijR3k-"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "print(unique_words[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrXOZdX9TUA5"
      },
      "source": [
        "#### **Q4b**\n",
        "\n",
        "Extract respective vectors corresponding to the words in your corpus and store the vectors in a variable called `vector_list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLj0yGxWT57d"
      },
      "source": [
        "#code here\n",
        "vector_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIGrdKaAUFNR"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "print(f'Array Shape: {np.array(vector_list).shape}')\n",
        "for i in range(5):\n",
        "    print(unique_words[i], vector_list[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE3v8fsvNZ00"
      },
      "source": [
        "### **Q5** (3 points)\n",
        "Based on your knowledge and understanding of the text corpus you have chosen, **form 3 hypotheses** of analogies or relationships (between words) that you expect will hold and **give a reason why. Experimentally validate these hypotheses** using similarity of the word vectors.\n",
        "\n",
        "**Example**: If using Moby Dick as the corpus, one hypothesis might be that the whale, \"Moby Dick\" is (cosine) more similar to \"fate\" than to \"evil\" because Moby Dick is symbolic of the nature and the universe and isn't necessarily 'bad'. Or \"Moby Dick\" is more similar to \"opposition\" than to \"surrender\" because Moby Dick fights for its survival.\n",
        "\n",
        "Note: Please do NOT use the same example as in the prompt.\n",
        "\n",
        "Note 2: It's okay if the model disproves your hypotheses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vFYo3XZ9wtE"
      },
      "source": [
        "---Your hypotheses here---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "a771483fbb59086604eb84bcc7c7f0ad",
          "grade": false,
          "grade_id": "cell-3aba86afc0ebd8a8",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "zQGd-YVoKDV3"
      },
      "source": [
        "#your code here for validating hypotheses 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "17796eb5de342e8f8e841aa137a2c41c",
          "grade": true,
          "grade_id": "cell-15ffa50b82de21ad",
          "locked": true,
          "points": 1,
          "schema_version": 1,
          "solution": false
        },
        "id": "HsSg0l2UKDV6"
      },
      "source": [
        "#your code here for validating hypotheses 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpWIoQPEaOyU"
      },
      "source": [
        "#your code here for validating hypotheses 3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJvNN_FcMxW"
      },
      "source": [
        "### **Q6** Visualizing the trained vectors (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4NOcNrPp6-n"
      },
      "source": [
        "#### **Q6a**\n",
        "\n",
        "Run K-means clustering on your word vectors (as you did in Q-6 of Lab-5). Use the word vectors from the model you trained in this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqlWClvlcL6T"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAUFEe8MpsLB"
      },
      "source": [
        "#### **Q6b**\n",
        "Reduce the dimensionality of your word vectors using TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuCBMydYpt-k"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF7wPyVsqDhd"
      },
      "source": [
        "#### **Q6c**\n",
        "\n",
        "#### Create a dataframe with the following columns:\n",
        "\n",
        "| Column |Description|\n",
        "| --- | --- |\n",
        "| x | the first dimension of result from TSNE |\n",
        "| y | the second dimension of result from TSNE |\n",
        "| Feature 1 | the word corresponding to the vector |\n",
        "| Feature 2 | the kmeans cluster label |\n",
        "\n",
        "<br>\n",
        "\n",
        "Below is a sample of what the dataframe could look like:\n",
        "\n",
        "|x\t|y|\tFeature 1|\tFeature 2|\n",
        "| --- | --- | --- | --- |\n",
        "|\t7.154159\t|9.251100|\tlips\t|8|\n",
        "|\t-53.254147|\t-13.514915|\tthem|\t9|\n",
        "|\t34.049191\t|-13.402843|\ttopic|\t0|\n",
        "|\t-32.515320|\t28.699677|\tsofa|\t24|\n",
        "|\t13.006302\t|-4.270626|\thalf-past|\t21|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjdPZ36vyvbQ"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6txdmyG3sNyd"
      },
      "source": [
        "#### **Q6d: Visualization**\n",
        "\n",
        "In this question, you are required to visualize and explore the reduced dataset you created in Q6c using the [d3-scatterplot](https://github.com/CAHLR/d3-scatterplot) library.\n",
        "\n",
        "Note: The first code-chunk at the top in this notebook clones the library from github. Make sure that it has been executed properly before you proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxNaQd0cqqUK"
      },
      "source": [
        "##### Save your dataset as a tsv file 'd3-scatterplot/mytext_new.tsv'\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "df.to_csv('d3-scatterplot/mytext_new.tsv', sep='\\t', index=False)\n",
        "```\n",
        "\n",
        "\n",
        "Note 1: The TSV file needs to be stored in the `d3-scatterplot` folder so that the d3-scatterplot library can access it.\n",
        "\n",
        "Note 2: Make sure to save the TSV file WITHOUT the row index i.e. use `index=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K5gzbguqIPd"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84qwcZkcuOd0"
      },
      "source": [
        "##### Visualize the reduced vectors by running the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPZaolUVuUk_"
      },
      "source": [
        "def show_port(port, data_file, width=600, height=800):\n",
        "  display(Javascript(\"\"\"\n",
        "  (async ()=>{\n",
        "    fm = document.createElement('iframe')\n",
        "    fm.src = await google.colab.kernel.proxyPort(%d) + '/index.html?dataset=%s'\n",
        "    fm.width = '90%%'\n",
        "    fm.height = '%d'\n",
        "    fm.frameBorder = 0\n",
        "    document.body.append(fm)\n",
        "  })();\n",
        "  \"\"\" % (port, data_file, height)))\n",
        "\n",
        "port = 8001\n",
        "data_file = 'mytext_new.tsv'\n",
        "height = 1400\n",
        "\n",
        "get_ipython().system_raw('cd d3-scatterplot && python3 -m http.server %d &' % port)\n",
        "show_port(port, data_file, height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFgOGcc-qAMw"
      },
      "source": [
        "Color the points by their kmeans cluster. You can do this by choosing \"Feature 2\" as the variable in the \"Color\" drop-down in the visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5eYErX6Zysg"
      },
      "source": [
        "Please include a snapshot of your visualization in the notebook. Refer to the tutorial video, and/or follow the steps below:\n",
        "\n",
        "1) Take a snapshot of the visualization and save it on your computer with the filename `trained_scatter.png`\n",
        "\n",
        "2) Upload the `trained_scatter.png` by clicking on the 'Files' icon on the left sidebar and clicking on the upload icon (the one with an upward arrow on top left)\n",
        "\n",
        "3) Run the code below to display the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmpvkhmZpeH7"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "Image('trained_scatter.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jb-GpNWrtgS"
      },
      "source": [
        "### **Q7** Visualizing the PRE-TRAINED vectors (1.5 points)\n",
        "\n",
        "In this question, you'll execute the same analysis as in Q6, but on PRE-TRAINED vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59DeKBftotQ"
      },
      "source": [
        "#### **Q7a**\n",
        "\n",
        "Load the google vector model\n",
        "\n",
        "(It must be downloaded as `GoogleNews-vectors-negative300` for you if you ran the first code-chunk at the top of this notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBkHkXDzuEOP"
      },
      "source": [
        "# google_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAgNt4e1uGCc"
      },
      "source": [
        "Downsample the pre-trained google model to anywhere between 10,000 to 25,000 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te4ilACpukal"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNbfhK26ulL2"
      },
      "source": [
        "Create a list of the unique set of words from this downsampled model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFqDd4kourbk"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXSlcxtur9E"
      },
      "source": [
        "Extract respective vectors corresponding to the words in the down-sampled, pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z9jchHku25j"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvmUumnDrtgT"
      },
      "source": [
        "#### **Q7b**\n",
        "\n",
        "Run Kmeans clustering on the pre-trained word vectors. Make sure to use the word vectors from the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAb4_l0QrtgT"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LobYFUfYrtgU"
      },
      "source": [
        "#### **Q7c**\n",
        "Reduce the dimensionality of the word vectors from the pre-trained model using tSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4hOre8MrtgU"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDfV8evgrtgU"
      },
      "source": [
        "#### **Q7d**\n",
        "\n",
        "#### Create a dataframe with the following columns using the pre-trained vectors and corpus:\n",
        "\n",
        "| Column |Description|\n",
        "| --- | --- |\n",
        "| x | the first dimension of result from TSNE |\n",
        "| y | the second dimension of result from TSNE |\n",
        "| Feature 1 | the word corresponding to the vector |\n",
        "| Feature 2 | the kmeans cluster label |\n",
        "\n",
        "<br>\n",
        "\n",
        "Below is a sample of what the dataframe could look like:\n",
        "\n",
        "|x\t|y|\tFeature 1|\tFeature 2|\n",
        "| --- | --- | --- | --- |\n",
        "|\t7.154159\t|9.251100|\tlips\t|8|\n",
        "|\t-53.254147|\t-13.514915|\tthem|\t9|\n",
        "|\t34.049191\t|-13.402843|\ttopic|\t0|\n",
        "|\t-32.515320|\t28.699677|\tsofa|\t24|\n",
        "|\t13.006302\t|-4.270626|\thalf-past|\t21|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulUE9IzE2u7O"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUb5tEIIrtgV"
      },
      "source": [
        "#### **Q7e: Visualization**\n",
        "\n",
        "In this question, you are required to visualize and explore the reduced dataset **from the pretrained model** you created in Q7d using the [d3-scatterplot](https://github.com/CAHLR/d3-scatterplot) library.\n",
        "\n",
        "Note: The first code-chunk at the top in this notebook clones the libary from github. Make sure that it has been executed before you proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecu9HXk_rtgV"
      },
      "source": [
        "##### Save your dataset as a tsv file 'd3-scatterplot/google_mytext.tsv'\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "google_df.to_csv('d3-scatterplot/google_mytext.tsv', sep='\\t', index=False)\n",
        "```\n",
        "\n",
        "\n",
        "Note 1: The TSV file needs to be stored in the `d3-scatterplot` folder so that the d3-scatterplot library can access it.\n",
        "\n",
        "Note 2: Make sure to save the TSV file WITHOUT the row index i.e. use `index=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE-2ug_ErtgV"
      },
      "source": [
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bxi9g_jrtgV"
      },
      "source": [
        "##### Visualize the reduced vectors by running the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIVpBRgjrtgV"
      },
      "source": [
        "def show_port(port, data_file, width=600, height=800):\n",
        "  display(Javascript(\"\"\"\n",
        "  (async ()=>{\n",
        "    fm = document.createElement('iframe')\n",
        "    fm.src = await google.colab.kernel.proxyPort(%d) + '/index.html?dataset=%s'\n",
        "    fm.width = '90%%'\n",
        "    fm.height = '%d'\n",
        "    fm.frameBorder = 0\n",
        "    document.body.append(fm)\n",
        "  })();\n",
        "  \"\"\" % (port, data_file, height)))\n",
        "\n",
        "port = 8001\n",
        "data_file = 'google_mytext.tsv'\n",
        "height = 1400\n",
        "\n",
        "get_ipython().system_raw('cd d3-scatterplot && python3 -m http.server %d &' % port)\n",
        "show_port(port, data_file, height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YnVGHBzpwVi"
      },
      "source": [
        "Color the points by their kmeans cluster. You can do this by choosing \"Feature 2\" as the variable in the \"Color\" drop-down in the visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW4vLM_YrtgW"
      },
      "source": [
        "Please include a snapshot of your visualization in the notebook. Refer to the tutorial video, and/or follow the steps below:\n",
        "\n",
        "1) Take a snapshot of the visualization and save it on your computer with the filename `google_trained_scatter.png`\n",
        "\n",
        "2) Upload the `google_trained_scatter.png` by clicking on the 'Files' icon on the left sidebar and clicking on the upload icon (the one with an upward arrow on top left)\n",
        "\n",
        "3) Run the code below to display the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPAJsbJMrtgW"
      },
      "source": [
        "#This is an autograded cell, do not edit/delete\n",
        "Image('google_trained_scatter.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkZXaOBmuYDs"
      },
      "source": [
        "### **Q8: Exploration** (0.5 points)\n",
        "\n",
        "This is an open-ended question.\n",
        "\n",
        "On the visualizations in Q6 & Q7, lasso-select a group of points with the left mouse button and look at summaries of the group on the right-side of the plot. (Refer to the tutorial video for a demo on the lasso selection). Also look at the words / features of the selected points.\n",
        "\n",
        "Comment on at least 3 patterns / similarities you see in the selected words in the visualization for the pre-trained vectors and the vectors trained on your corpus. Are you able to find any group of points that are close to each other in the 2D space that also have semantic similarity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEDE7G-wyAU7"
      },
      "source": [
        "--your answer here--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAVOMlN2x-jm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}